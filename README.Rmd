---
title: "長庚大學 大數據分析方法 作業三"
output: github_document
---



    

## 網站資料爬取
```{r}
#這是R Code Chunk
 ##第一次使用要先安裝 install.packages("rvest")
##install.packages("rvest")
library(rvest)



HTML <-read_html("https://www.ptt.cc/bbs/Tech_Job/index.html")
btnURL<-HTML %>% html_nodes("a[class='btn wide']") %>% html_attr("href")
indexNum <- as.numeric(gsub("/bbs/Tech_Job/index|.html"," ",btnURL[2]))
 
totalPTT<-NULL
  
 
for (i in (indexNum-4):(indexNum+1)){
    #print(i)
    HTML=paste("https://www.ptt.cc/bbs/Tech_Job/index",i,".html",sep="")
    Title <- read_html(HTML) %>%html_nodes(".title") %>% html_text()
   
    PushNum<-read_html(HTML) %>%html_nodes(".nrec") %>% html_text()
    
    Author<-read_html(HTML) %>%html_nodes(".author") %>% html_text()
   
   
   
    

    PTT<-data.frame(Title,PushNum,Author)
   #dataframeAll<-rbind(dataframe1,dataframe2) 
    totalPTT<-rbind(totalPTT,PTT)
 
 
   
   }
       View(totalPTT)
 
    
 
 

```

## 爬蟲結果呈現
```{r}
#這是R Code Chunk
knitr::kable(totalPTT)
```

## 解釋爬蟲結果 
```{r}
dim(totalPTT)


```

↑總共爬出108筆文章標題

```{r}

max(
    table(totalPTT$Author)

)

 table(totalPTT$Author)
```

↑最多篇文章顯示九篇，作者顯示為"-"，
回版上看發現作者為"-"是已被刪除的文章.....
而第二多的是3篇，作者為"wer11"

↑人工結論
最有趣的地方是同意作者最多文章數竟然是被刪除的文章，另外工作版感覺不錯看呢!發現不可以面試放鳥公司，不然會被萬年黑名單
